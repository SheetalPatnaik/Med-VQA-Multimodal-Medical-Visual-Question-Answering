# ğŸ¥ Med-VQA â€” Multimodal Medical Visual Question Answering

Hi, Iâ€™m **Sheetal Patnaik** ğŸ‘‹  

This repository contains my end-to-end **Medical Visual Question Answering (Med-VQA)** system.  
I designed this project to blend **vision-language models (VLMs)** with **Retrieval-Augmented Generation (RAG)** and **advanced prompting strategies (Chain-of-Thought, Tree-of-Thought, Few-shot prompting)**.  

My goal: build a Med-VQA system that is **accurate, explainable, and clinically meaningful**.  

---

## ğŸ”‘ Key Contributions
- **Multimodal RAG Pipeline** â†’ integrates visual embeddings (CLIP) and biomedical text retrieval (PubMedQA, SLAKE) using FAISS.  
- **Prompting Strategies** â†’ Zero-shot, Few-shot, **CoT**, **ToT**. CoT/ToT achieved **77% recall**, outperforming fine-tuned baselines.  
- **LoRA Fine-Tuning** â†’ Adapted **LLaVA-Med (7B)** using efficient fine-tuning (r=4, Î±=16, dropout=0.05). Baseline accuracy ~54%.  
- **Answer Validation Loop** â†’ enforced structured outputs (A/B/C/D) and re-asked when uncertain responses appeared.  
- **Explainability with MedCoT** â†’ introduced structured reasoning traces for more transparent clinical AI.  

---

## ğŸ§ª Datasets Used
- **PMC-VQA** â†’ 227k QA pairs from 149k medical images (X-ray, MRI, CT, pathology).  
- **SLAKE** â†’ 642 medical images, 14k QA pairs (used for retrieval).  
- **PubMedQA** â†’ biomedical literature dataset with yes/no/maybe answers.  
- Benchmarks: **VQA-RAD, PathVQA, Med-VQA 2019, ImageCLEF-VQA**.  

> âš ï¸ Datasets are not included in this repo. Please download from their official sources.

---

## âš™ï¸ Tech Stack
- **Models**: LLaVA-Med, GPT-4 (prompting), CLIP ViT-B/32, PMC-CLIP  
- **Libraries**: PyTorch, Transformers, Sentence-Transformers, FAISS, OpenAI/CLIP  
- **Paradigms**: RAG, Chain-of-Thought, Tree-of-Thought, Few-shot, LoRA Fine-Tuning  

---


ğŸ‘‰ **Prompting + Retrieval > Fine-Tuning** for generalization and interpretability.

---

## ğŸ“‚ Repository Contents
- **`/notebooks`** â†’ Implementation notebooks (RAG pipeline, prompting, LoRA fine-tuning).  
- **`/reports`** â†’ Final report & proposal (detailed methodology, evaluation, and limitations).  
- **`/slides`** â†’ Presentation deck.  
- **`/data`** â†’ Placeholder for dataset instructions (no raw data).  

---

## â–¶ï¸ Quickstart
```bash
# 1. create environment
python -m venv .venv
source .venv/bin/activate

# 2. install dependencies
pip install -r requirements.txt

# 3. run notebooks
jupyter lab
